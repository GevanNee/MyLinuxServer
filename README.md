
# Linux通用服务器框架

> 主要参考书籍：《C++新经典---Linux C++通信架构实战》 作者：王健伟，请多支持原作者
其他参考：
> >linux/unix编程手册
> >nginx源码
> >博客：<https://www.subingwen.cn/> by-苏丙温

- [Linux通用服务器框架](#linux通用服务器框架)
  - [前言](#前言)
  - [一，进程线程结构](#一进程线程结构)
  - [二，基础设施](#二基础设施)
    - [1，日志模块](#1日志模块)
      - [效果](#效果)
      - [实现方法](#实现方法)
    - [2，配置文件（单例不加锁）](#2配置文件单例不加锁)
      - [格式：直接看配置文件本文件](#格式直接看配置文件本文件)
      - [实现](#实现)
    - [3，改进程名](#3改进程名)
      - [实现方法 : 略](#实现方法--略)
    - [4.守护进程](#4守护进程)
    - [5.信号处理](#5信号处理)
  - [三，通信模块](#三通信模块)
    - [1.用一个结构体来描述一个TCP连接](#1用一个结构体来描述一个tcp连接)
    - [2.为上述结构体分配一个池(std::list)](#2为上述结构体分配一个池stdlist)
    - [3.为上述结构体分配一个空闲连接池(std::list)](#3为上述结构体分配一个空闲连接池stdlist)
    - [4.用一个大类管理整个进程的连接状态](#4用一个大类管理整个进程的连接状态)
    - [5.socket流程](#5socket流程)
    - [6.epoll(LT)](#6epolllt)
      - [LT和ET](#lt和et)
    - [7.自定义事件处理方法](#7自定义事件处理方法)
    - [8.收包](#8收包)
      - [维护收包状态](#维护收包状态)
    - [其他](#其他)
      - [IO的阻塞非阻塞异步同步](#io的阻塞非阻塞异步同步)
      - [大端小端](#大端小端)
  - [四，业务逻辑模块(线程池处理业务逻辑，单独线程发包)](#四业务逻辑模块线程池处理业务逻辑单独线程发包)
    - [1.线程池](#1线程池)
      - [线程池的属性](#线程池的属性)
      - [条件变量和信号量](#条件变量和信号量)
        - [虚假唤醒](#虚假唤醒)
        - [唤醒丢失](#唤醒丢失)
      - [线程个数给多少](#线程个数给多少)
    - [2.从收包完毕到开始处理业务逻辑](#2从收包完毕到开始处理业务逻辑)
      - [业务逻辑](#业务逻辑)
    - [3.发包](#3发包)
  - [五，应用层服务器安全](#五应用层服务器安全)
    - [1.应用层flood攻击](#1应用层flood攻击)
    - [2.畸形数据包](#2畸形数据包)
    - [2.控制连接数量](#2控制连接数量)
    - [3.超时踢出(心跳包(可选)，或者需要限制连接时长(可选))](#3超时踢出心跳包可选或者需要限制连接时长可选)
      - [把TCP连接再加入一个时间队列](#把tcp连接再加入一个时间队列)
    - [4.延迟回收(重点)](#4延迟回收重点)
      - [为什么要有延迟回收？](#为什么要有延迟回收)
      - [延迟回收的线程](#延迟回收的线程)
  - [六，makefile](#六makefile)
  - [七，关于测试](#七关于测试)

## 前言

写这玩意几个目的：

一是复盘一下全部代码，不然写完下面忘了上面，打再多注释也会忘记。

二是看看自己有没有能力做技术分享，35岁以后失业可能就靠这个吃饭了(只有十年了)

## 一，进程线程结构

- 一个master进程，n个worker进程,(可以通过配置文件改进程个数)。
- worker进程的主线程处理网络IO
- 每个worker进程有一个线程池处理业务逻辑。
- 另外三个特殊线程(也附属于worker进程)：
  - 处理写事件的线程
  - 检测超时连接的线程
  - 负责延迟回收连接的线程
- **master进程的职责**：由信号或者时钟事件驱动，管理worker进程，可以做到整个服务器优雅退出(捕捉SIGQUIT信号并且发给worker进程)，或者热更新(不知道nginx怎么做到的)？或者子进程意外死亡再重新抬一个(SIGCHILD信号还需要完善)？目前只实现了回收子进程。

## 二，基础设施

### 1，日志模块

#### 效果

![日志输出演示.png](https://github.com/GevanNee/MyLinuxServer/blob/master/ReadmeSource/%E6%97%A5%E5%BF%97%E8%BE%93%E5%87%BA%E6%BC%94%E7%A4%BA.png)

- 自动生成时间，日志等级和输出日志的进程编号，后面的内容是自己解析的格式化输出，类似printf这种。
- 两个函数，一个写文件，一个写屏幕(开启守护进程后失效)：
    ![日志函数调用演示.png](https://github.com/GevanNee/MyLinuxServer/blob/master/ReadmeSource/%E6%97%A5%E5%BF%97%E8%B0%83%E7%94%A8%E6%BC%94%E7%A4%BA.png)
- 还可以这样，格式化输出：
    ![格式化输出.png](https://github.com/GevanNee/MyLinuxServer/blob/master/ReadmeSource/%E6%97%A5%E5%BF%97%E6%A0%BC%E5%BC%8F%E5%8C%96%E8%BE%93%E5%87%BA.png)
- 如果errno不为零，后面还会跟一个括号，打印出errno对应的字符串信息，比如：
 ![errno信息.png](https://github.com/GevanNee/MyLinuxServer/blob/master/ReadmeSource/errno%E6%BC%94%E7%A4%BA.png)

#### 实现方法

- 时间用 `gettimeofday()`  生成历史秒钟数，再用 `localtime_r(&second, &tm)` 转化成年月日格式
- 然后日志等级+进程编号( `getpid()` )
- 后面的内容通过百分号来判定是否进行替换。可以解析%02d, %l, %f, %P，等等，需要什么再添加。具体过程请看ngx_printf.cpp文件里的ngx_sprintf函数。

### 2，配置文件（单例不加锁）

#### 格式：直接看配置文件本文件

#### 实现

- 文件按行读取，过滤掉注释，块说明等无关内容。解析出来的key和value存起来，然后由两个接口： `getint()` 和 `getstring()` 取出。
- 按理来说单例模式应该加双检查锁保证线程安全(或许还有reorder问题)，但是同时满足以下条件就能不加锁：
  - 对该模块只读
  - 保证在进程启动后，创建多线程前将它初始化
  - 初始化失败就退出整个进程

### 3，改进程名

- 启动进程的二进制文件名字和ps命令看到的名字一般是一样的
- 但是我们可以修改这个名字
 ![二进制文件名字.png](https://github.com/GevanNee/MyLinuxServer/blob/master/ReadmeSource/%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6.png)
 ![ps命令看到的名字.png](https://github.com/GevanNee/MyLinuxServer/blob/master/ReadmeSource/ps%E5%91%BD%E4%BB%A4.png)
 可以看到ps命令出来的woker进程和master进程名字都不一样。说明被我修改过的。

- 视频演示：

#### 实现方法 : 略

### 4.守护进程

- 这个简单，搞清楚会话，进程组这些玩意儿就行。
- 主要是调用`setsid()` (进程组的组长无法调用这个函数)， 然后改文件权限，然后关闭STDIN STDOUT STDERR就行。具体看ngx_daemon.cpp，基本所有的守护进程方法都这样写。

### 5.信号处理

- 主要函数 `sigpromask()`，`sigempty()` 等等，具体看signal.cpp，自定义需要处理的信号集。
- 目前只处理了master进程收到的，SIGCHILD信号，其他信号待添加。
- main函数中用SIGCHILD信号同时回收多个子进程：(具体代码看ngx_signal.cpp文件里的 `ngx_process_get_status()` 函数)
 ![回收子进程演示.png](https://github.com/GevanNee/MyLinuxServer/blob/master/ReadmeSource/%E5%9B%9E%E6%94%B6%E5%AD%90%E8%BF%9B%E7%A8%8B.png)

## 三，通信模块

### 1.用一个结构体来描述一个TCP连接

- 用户态通过一个文件描述符来标识一个TCP连接(通过accept得到的)，把这个文件描述符和一些信息绑定起来，得到一个TCP连接的结构体
- 通过这个结构体我们就能管理这个tcp连接的所有状态，包括：收包状态(维护一个状态机)，序号(用来标识过期连接)，读事件处理方法，写事件处理方法等等
- 具体查看源码里的ngx_socket.h文件里的 `struct ngx_connection_s{ ... }` 结构体，里面有详细的注释

### 2.为上述结构体分配一个池(std::list)

- 有了这玩意，高并发下频繁的连接和断开，就不必再给一个连接重新分配内存了，服务器初始化的时候就把内存给安排好了。
- 用std::list保存该TCP结构体的指针，然后TCP结构体放在堆内存里管理。就是注意std::list析构的时候，要先把TCP结构体给释放了。

### 3.为上述结构体分配一个空闲连接池(std::list)

- 空闲连接池是为了在新连接到达的时候，快速的从连接池里找一个空闲的连接来用，实现也很简单，也是一个std::list存该TCP结构体的指针。
- 连接池的list是基本不动的，除非连接到达上限，可以选择不同的策略(添加连接或者拒绝连接)。空闲连接池的连接是随时变化的，由于只涉及指针操作，所以新连接到来时可以极快的速度给新连接分配TCP结构体。
- 也可以在TCP结构体里添加一个next指针成员来维护空闲连接，这样做的话比较麻烦，还是std::list比较方便。

### 4.用一个大类管理整个进程的连接状态

- 这个类提供了所有管理连接的方法，具体看ngx_socket.h文件里的 `class CSocket` 类，里面也有详细的注释。

### 5.socket流程

- socket() bind() listen() 流程也很简单，注意要设置socketfd为REUSEADDR REUSEPORT，并且搞成非阻塞。具体代码见ngx_c_socket.cpp文件的 `bool CSocket::ngx_open_listening_sockets()` 函数
- REUSEADDR为了解决TCP连接TIME_WAIT状态时bind失败(相同五元组还是会失败)，REUSEPORT解决惊群问题(需要内核支持)，总之服务器代码都加上没毛病。
- 可以通过配置文件决定监听端口数，监听完毕后要把监听fd加入到连接池里，占一个坑。

### 6.epoll(LT)

- epoll原理也不复杂，在内核注册回调函数，用 `eopll_ctl()` 管理事件(往红黑树上加，更改，删除)，事件就绪往就绪队列上添加(指针操作)，`epoll_wait()` 拿下事件返回给用户态，用户遍历epoll_wait的返回值就行了。

#### LT和ET

- 一句话描述，LT和ET就是内核 把红黑树上的节点往就绪链表上放 的策略。
- 原理也很简单，LT和ET的英文单词解释得明明白白
- LT(level trigger)水平触发，也就是说事件就绪时，用来标记事件就绪状态得二进制位处于高电平，那么内核就一直会把该事件往就绪链表里塞。
- ET(edge trigger)边沿触发，事件状态从低电平变为高电平的一瞬间，内核才会把该事件往就绪链表上放，除此之外任何情况不会放。
- 两种模式对程序员编程影响很大，ET的话，用户态没把事件处理完(比如一次性读完)，那么下次就不可能通过epoll来触发读事件了。只能等新的数据过来

### 7.自定义事件处理方法

- epoll_event结构体长这个样子：

```c++
struct epoll_event
{
  uint32_t events; /* Epoll events */
  epoll_data_t data; /* User data variable */
} __EPOLL_PACKED;

typedef union epoll_data
{
  void *ptr;
  int fd;
  uint32_t u32;
  uint64_t u64;
} epoll_data_t;
```

- 注意epoll_data结构是一个union，一般来说用其中的fd就行，这里可以用void*指针，来指向该事件对应的TCP结构体，然后TCP结构体中有两个回调函数：

```c++
typedef void (CSocket::*ngx_event_handler_pt)(lpngx_connection_t c); //定义成员函数指针

struct ngx_connection_s
{
    int fd;
    ......

    ngx_event_handler_pt r_handler;  //读事件的相关处理办法
    ngx_event_handler_pt w_handler;  //写事件

    ......
}
```

- 这样就能在调用epoll_ctl的ADD方法时，自己指定一个处理方法。
- 回调函数具体在哪里执行？请看ngx_socket.cpp文件里的ngx_epoll_process_events()函数的这行：

```c++
(this->*(p_Connect->r_handler) )(p_Connect);
```

- 解释一下这个语法：`*(p_Connect->r_handler)`，拿到了回调函数的地址，这个回调函数是成员函数，通过 `this->(函数地址)(参数)` 就可以执行这个函数。
- 目前对于listenfd，它的读事件处理函数就是ngx_event_accept()
- 对于一个用户连接，它的读事件处理就是ngx_event_request()

### 8.收包

- 收包不得不提TCP粘包，直接原因是TCP面向字节流，才不管你要发什么包。根本原因是上层协议没做好(TCP:才不是我的锅！)，没有分清楚包的界限。
- 解决方法有很多，定义包头，或者用特殊字符序列来表示包尾巴(比如HTTP的头部的/r/n/r/n)。
- 这里我们用自定义包头的形式(自定义协议)，和客户端商量(sibi)好，大家都用这个格式的包头来收发包。

#### 维护收包状态

- 刚开始就盯着包头来收，收几个字节，下次就少收点，直到把包头收完，包头收完之前不要去管包体。
- 由于一个包有包头和包体，但是我们调用read的时候不一定能一次收完包头，所以维护两个状态：尚未开始收包头，和包头接收中
- 收完包头就开始收包体，有些包没有包体(比如ping包)，包体也有两种状态，尚未开始收包体，和包体接收中。
- 这样我们就有四个状态，另外一个状态是接收完成，但是感觉接收完成就可以直接跳到一开始的尚未开始接收包头状态。
- 具体代码请看ngx_c_socket_request.cpp文件的 `ngx_read_request_handler()` 函数，里面的四个if分支维护了一个TCP结构体的收包状态
- 状态的定义在ngx_comm.h文件

### 其他

#### IO的阻塞非阻塞异步同步

- 网上八股文很多了，这里做个简单说明：
- 阻塞IO就是事件未就绪就等待
- 非阻塞IO就是事件未就绪就返回-1
- 同步IO就是自己做IO操作
- 异步IO就是让别人(其他进程，或者其他线程，或者内核)帮我做IO操作，做完然后通知我(SIGIO)。
- 写日志的write方法看似是同步IO，其实是内核帮你入盘。
- 想快速入盘的话用async吧。

#### 大端小端

- 只需要记得网络都是大端，单字节不用转换，就行了。
- 默认不知道自己机器是大端小端。

## 四，业务逻辑模块(线程池处理业务逻辑，单独线程发包)

### 1.线程池

#### 线程池的属性

- 直接看ngx_theadpool.h文件里theadpool类的成员变量，注释很详细。
- 一个线程结构体，任务队列，一个条件变量，一把互斥锁，一个线程结构体集合，等等。
- 可能还需要其他参数：比如线程池最大个数，线程池是否需要缩减等等，可以根据自己的需要再添加。

#### 条件变量和信号量

- 这俩玩意感觉一样，只是信号量多了进程间通信的功能。
- 非要说什么不同，语义不一样吧，条件变量偏向于流程控制，信号量偏向于资源个数。

##### 虚假唤醒

- 多核CPU的BUG，调用一次signal偶尔会唤醒两个线程(还不知道原理)，解决方法就是线程被唤醒后再判定一次就绪条件就行了。具体见ngx_c_theadpool.cpp文件的ThreadFunc()函数。

##### 唤醒丢失

- 问题真多，还不如信号量。
- 任务队列的生产者没加锁，活该丢失。
- 线程全部在忙时调用signal？不用管就行，有资源的话线程忙完会自动去拿。

#### 线程个数给多少

- 八股文说CPU密集，IO密集什么的。还给出什么公式，很有道理，但是我觉得怪怪的。
- 最终目的是吧CPU吃满，或者吃到70% ~ 90%，得根据自己的业务看，初始给个两倍CPU核心差不多，后面自己慢慢调。玩游戏这么久，CPU性能早就了如指掌，GPU都给你扒了。

### 2.从收包完毕到开始处理业务逻辑

- 收完一个包，必须判断合法性，目前包头定义了三个成员：包长(包头+包体)，消息码(处理什么业务逻辑)，CRC验证(验证收到的包和发送的包的数据是否一致)。
- 整个过程要经过：
  - 1.每次收完一个包都要算一下收包频率，是否有flood攻击，见第五章。
  - 2.包头中包长是否在合理范围内，必须规定包的最大长度，不然恶意包会把内存占满。(具体见 `ngx_wait_request_handler_proc_plast()函数` )
  - 3.如果是包头长度合理的包，那么把它添加到任务队列里，唤醒一个工作线程。 (具体见 `Call()` 函数)
  - 4.工作线程拿到这个包，交给业务处理模块，这个模块会对包的业务部分做进一步判断。(拿完包就不用互斥了) (具体见 `TestFlood()` 函数)
  - 5.业务模块拿到包，首先计算一下包体的crc值，然后要注意消息码，是否在我们规定的范围内。(具体见 `threadRecvProcFunc()函数` )
  - 6.都满足的话，可以通过消息码快速定位到需要处理的业务逻辑函数。
  - 7.业务逻辑函数继续对包体进行验证，包体长度
- 收完一个包后唤醒一个线程，唤醒一个线程后马上检测一次工作线程是否达到上限，达到上限的解决策略可以自己定。(具体见 `Call()` 函数)
- 特别说明 `threadRecvProcFunc()` 函数底部的这一行：
- `(this->*(statusHandler[imsgCode]))(pMsgHeader->pConn, pMsgHeader, (char*)pPkgBody, pkglen - m_iLenPkgHeader);`
- 这就是具体处理业务逻辑的函数

#### 业务逻辑

- 看后续需求，还没想好做什么业务。总之就是拿到包处理完然后发送包。

### 3.发包

- 用一个专门的线程来发包，线程池(业务逻辑)相当于发包任务的生产者，发包线程是消费者，用信号量驱动，注意对发送消息队列互斥。
- 该线程直接发就行，不用把写事件添加到epoll节点上，原因：
- 1.跟读事件不同，写事件可以默认写缓冲区可用，所以不用麻烦内核去通过epoll检测。
- 2.万一真的满了，这边写缓冲区满了，那么write就会返回-1，同时errno置为EAGIN。这时候再把写事件添加到epoll节点上就行，接下来就靠epoll来驱动发包。
- 具体代码看ngx_c_socket.cpp文件里的ServerSendQueueThread()函数
- 另外，发包线程绝对不能阻塞，或者进入死循环，看ServerSendQueueThread()函数里的一堆DEBUG日志就知道发生了什么。具体见(一个分号引起的灾难)。

## 五，应用层服务器安全

### 1.应用层flood攻击

- 配置文件中可以修改应对flood攻击的各种选项
- TCP结构体里维护了一个收到包时间，收到完整包后更新该时间，但是在这之前要先对比一下上次收包时间跟本次收包之间的间隔。
- 具体代码见 `TestFlood()` 函数

### 2.畸形数据包

- 比如传字符串，有些恶意包它不加'\0'，我们服务器这边一律把它加上。防止后续解析出问题。

### 2.控制连接数量

- 每次分配连接( `ngx_get_connection()` 函数)，都要检查一下是否还有空闲连接。
- 没有的话自己制定策略，可以拒绝分配连接，或者增加连接池大小。

### 3.超时踢出(心跳包(可选)，或者需要限制连接时长(可选))

- 有一个单独的线程，负责管理超时连接，超时连接一般就是心跳包时间超时(超过三倍心跳包时间)
- 另外，如果有控制每个连接的连接时长的需求，也是用该线程踢出(具体代码见 `ServerTimerQueueMonitorThread()` 函数)

#### 把TCP连接再加入一个时间队列

- 如果有这个需求，每个TCP连接在连入服务器的那一刻，就决定了它在服务器里的寿命(其实应该客户端自己断开)。
- 实现：把最终断开时间(或者加入时间)作为key，TCP连接本连接作为value，加到红黑树里(std::mutilmap，红黑树自排序),那么超时踢出的线程只需要看红黑树的头节点就行了。

### 4.延迟回收(重点)

#### 为什么要有延迟回收？

- 假设当前是立即回收连接(也就是关闭一个fd就把对应的TCP结构体加到空闲连接队列里)
- 设想以下场景：
- 1.业务逻辑处理时间很长
- 2.用户A连接上来，然后发送了一条命令给服务器处理业务逻辑，然后用户A立刻断开(无论什么原因)，主线程直接回收了该TCP连接
- 3.紧跟着，用户B连接上来，这时候用户B恰好复用了刚才用户A的TCP连接的内存。但是这时候工作线程还没反应过来，那么工作线程可能会修改自己拿到的TCP结构体的内存。
- 4.这时候用户B还玩个毛，我连进来是让你为我提供服务的，不是让你对我乱动的。垃圾服务器。
- 注意：nginx中用的是立即回收，用指向该连接的指针的最后一位来标识连接是否过期。()

#### 延迟回收的线程

- 回收时给一个回收时间，比如21:15:35时关闭了一个fd，那么给对应的TCP结构体里的inRecyTime成员赋值，然后把这个TCP结构体加入到回收队列里。
- 这个线程定时遍历这个队列就行了，把满足条件的((inRecyTime + m_RecyConnectionWaitTime) > currTime)踢出就行。
- 配置文件里可以对回收时间进行更改

## 六，makefile

- makefile待添加

## 七，关于测试

- 目前还没有对这个服务器框架尚未进行大规模测试(作者提供了简单测试客户端，但是我不想去看MFC的内容)，但是基本的收包和发包没问题，在连接数接近进程最大描述符(目前每个进程1000，还没改最大打开文件)，速度也是极快，epoll还是很给力的。
- 所以准备写一个QT客户端项目(有时间的话，搞个多人斗地主？或者云盘？)。
- 内存泄漏测试：准备用valgrind
- 压力测试：就用自己写的客户端测试
